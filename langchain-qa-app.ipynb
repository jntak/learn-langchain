{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81228a4e-5f97-442f-bfa5-f026e88762b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Desktop\\joshua ntak\\projects\\langchain-qa-app\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c3a526-24df-4f29-8570-d125cf4883f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Joshua Ntak\n",
      " Volume Serial Number is 8678-7943\n",
      "\n",
      " Directory of C:\\Users\\PC\\Desktop\\joshua ntak\\projects\\langchain-qa-app\n",
      "\n",
      "21-Sep-25  03:50    <DIR>          .\n",
      "21-Sep-25  03:50    <DIR>          ..\n",
      "19-Sep-25  23:24               295 .env\n",
      "20-Sep-25  09:05                27 .gitignore\n",
      "20-Sep-25  09:07    <DIR>          .ipynb_checkpoints\n",
      "21-Sep-25  03:50             6,506 langchain-qa-app.ipynb\n",
      "               3 File(s)          6,828 bytes\n",
      "               3 Dir(s)  40,302,088,192 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67da018b-9b0c-471a-bc9a-a529615cc46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True) \n",
    "\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e54220-c4dd-4a3b-9570-404bc5dc1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8408da6b-a1b8-4e3a-b474-23a441856475",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85d5cc1b-a287-4597-b42e-bee8618ddc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_backwards_compat_tool_calls', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', 'additional_kwargs', 'construct', 'content', 'copy', 'dict', 'example', 'from_orm', 'get_lc_namespace', 'id', 'invalid_tool_calls', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'parse_file', 'parse_obj', 'parse_raw', 'pretty_print', 'pretty_repr', 'response_metadata', 'schema', 'schema_json', 'text', 'to_json', 'to_json_not_implemented', 'tool_calls', 'type', 'update_forward_refs', 'usage_metadata', 'validate']\n"
     ]
    }
   ],
   "source": [
    "response = chat_llm.invoke(\"What is Quantum mechanics, in one sentence\")\n",
    "print(dir(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "044085eb-0a3d-4612-b99a-7d632ed4139b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles, where energy, momentum, angular momentum, and other quantities are quantized, not continuous.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27e6389a-f4c1-4259-9877-0acbb606e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(ChatGoogleGenerativeAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5ed1619-14ff-4133-ac23-f086a284fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, let's talk seismic interpretation. The *very* first step, before you even think about horizons or faults, is **data loading and QC (Quality Control).**\n",
      "\n",
      "Think of it like this: you wouldn't start building a house on a shaky foundation, would you? Same principle here. You need to make sure your seismic data is in good shape before you start interpreting anything.\n",
      "\n",
      "Here's what that typically involves:\n",
      "\n",
      "*   **Loading the data:** Getting the seismic volume into your interpretation software. This might involve dealing with SEG-Y files, coordinate systems, and making sure everything is properly georeferenced.\n",
      "*   **Visual inspection:** A thorough visual check of the data. Are there any obvious problems? Things like:\n",
      "    *   **Noise:** Is there excessive noise that might obscure real geological features?\n",
      "    *   **Amplitude issues:** Are there amplitude variations that don't seem geologically plausible? Are there dead traces or sections with poor data quality?\n",
      "    *   **Phase issues:** Is the data zero-phased or minimum-phased? Is the phase consistent throughout the volume?\n",
      "    *   **Polarity:** Is the data properly poled?\n",
      "*   **Basic data conditioning (if needed):** If you spot some issues, you might do some basic data conditioning to improve the data quality. This could involve things like:\n",
      "    *   **Noise reduction filters:** Applying filters to reduce random noise.\n",
      "    *   **Amplitude balancing:** Applying gain functions to balance amplitudes across the volume.\n",
      "    *   **Trace editing:** Removing or interpolating bad traces.\n",
      "\n",
      "**Why is this so important?**\n",
      "\n",
      "Because garbage in, garbage out! If you start interpreting on bad data, you're going to end up with a flawed interpretation. You might pick horizons that aren't real, miss faults, or misinterpret geological features.\n",
      "\n",
      "So, before you even think about picking that first horizon, make sure your data is as clean and reliable as possible. It'll save you a lot of headaches down the road.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    SystemMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a Geophysicist, Speak like a seismic interpreter\"),\n",
    "    HumanMessage(content=\"What is the very first step of seismic interpretation\")\n",
    "]\n",
    "\n",
    "\n",
    "response = chat_llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb4f8a-d43a-4add-92c0-821cecdee58c",
   "metadata": {},
   "source": [
    "# Caching LLM Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c6154-0ba1-476b-b3ab-ee1efb245f8f",
   "metadata": {},
   "source": [
    "## In-memory Cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7727cd4-fca1-45f0-856f-5b48848130e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import langchain_google_genai\n",
    "# help(langchain_google_genai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa5e44c4-404d-4ccb-8f43-42627ddabd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72b8e5d3-c2a3-42da-b0a2-154de10bdeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Governor of Rivers State is Siminalayi Fubara.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = \"Who is the Governor of Rivers State\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "262ec459-de6c-4c20-a6da-66db3edb5717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Governor of Rivers State is Siminalayi Fubara.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf762e71-e8fd-4129-8e9e-eb0b32934ca3",
   "metadata": {},
   "source": [
    "## SQLite Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad1b3c2b-b675-476a-8de5-a75216f3dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 12 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The President of the United States of America is **Joe Biden**.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "prompt = \"Who is the President of the United States of America\"\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8945a80d-b458-428d-8eee-a2e3c7e96c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The President of the United States of America is **Joe Biden**.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bf2d3a8f-7ac8-467b-8fb2-b31826a4b546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The UEFA Champions League final in 2025 has not happened yet!\\n\\nThe tournament for the 2024-2025 season will conclude in the spring of 2025, and the winner will only be known after that match is played.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain.globals import set_llm_cache\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm_knowledge = GoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=1\n",
    ")\n",
    "llm_knowledge.invoke(\"What club won the UEFA Champoin's League in 2025 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fcd6e-95cb-4072-b653-e9fbb54d2d70",
   "metadata": {},
   "source": [
    "# LLM Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6fcaef2a-f9b4-40d7-9bd5-702668446cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49987b1a-0ce2-48d6-8a76-ac49ca964198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald John Trump was the 45th President of the United States, serving from 2017 to 2021. Before entering politics, he was a businessman and television personality.\n",
      "\n",
      "Born in Queens, New York City, in 1946, Trump graduated from the Wharton School of the University of Pennsylvania with a degree in economics. He then joined his father's real estate and construction firm, Elizabeth Trump and Son, which he later renamed The Trump Organization. Over the decades, he built, renovated, and managed numerous skyscrapers, hotels, casinos, and golf courses. He also hosted the reality television show \"The Apprentice.\"\n",
      "\n",
      "Trump's political career began in 2015 when he announced his candidacy for President of the United States as a Republican. His campaign focused on issues such as immigration, trade, and national security. He won the 2016 presidential election, defeating Hillary Clinton.\n",
      "\n",
      "During his presidency, Trump enacted significant tax cuts, appointed numerous conservative judges, and pursued deregulation policies. He also initiated trade disputes with China and other countries, withdrew the United States from the Trans-Pacific Partnership trade agreement and the Paris Agreement on climate change, and moved the U.S. embassy in Israel to Jerusalem.\n",
      "\n",
      "Trump was impeached twice by the House of Representatives, once in 2019 for abuse of power and obstruction of Congress, and again in 2021 for inciting an insurrection. He was acquitted by the Senate in both trials.\n",
      "\n",
      "Since leaving office, Trump has remained active in politics and continues to be a prominent figure in the Republican Party.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short biography of President Donald Trump\"\n",
    "\n",
    "for chunk in chat_llm.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94985a-305a-4868-92a6-905162708e35",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f143ec9-f4c9-420b-9c16-f63d823c5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "template = \"\"\"\n",
    "You are an experienced Geophysicist,\n",
    "Explain the following concept: {concept} in only {nos} sentences, but in clear terms\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(concept=\"Seismic-To-Well Tie\", nos=\"three\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "993b1d7c-bbbc-454b-8706-d9742a819686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(prompt):\n",
    "    res = chat_llm.invoke(prompt)\n",
    "    print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c597166d-57dd-40d2-bd76-829439c09cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seismic-to-well tie is the process of correlating seismic data with well log data to accurately determine the depth of subsurface geological formations. This involves creating a synthetic seismogram from well logs and matching it to the real seismic data, allowing geoscientists to calibrate seismic reflections to specific geological layers encountered in the well. The result is a more accurate interpretation of the seismic data, leading to better reservoir characterization and drilling decisions.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265b462-285d-461d-ba5e-98ed05397705",
   "metadata": {},
   "source": [
    " # ChatPrompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6592aee9-7725-43ab-a137-ce898db126e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Respond only in JSON fomat', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Give top 10 countries in the world with lowest cost of living', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='Respond only in JSON fomat'),\n",
    "        HumanMessagePromptTemplate.from_template(\"Give top {n} countries in the world with {degree} cost of living\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n=\"10\", degree=\"lowest\")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be27e2d8-ea87-4df2-bd4a-a6d725392717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"top_10_countries_lowest_cost_of_living\": [\n",
      "    {\n",
      "      \"rank\": 1,\n",
      "      \"country\": \"Pakistan\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 2,\n",
      "      \"country\": \"Nepal\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 3,\n",
      "      \"country\": \"Syria\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 4,\n",
      "      \"country\": \"India\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 5,\n",
      "      \"country\": \"Afghanistan\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 6,\n",
      "      \"country\": \"Bangladesh\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 7,\n",
      "      \"country\": \"Egypt\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 8,\n",
      "      \"country\": \"North Macedonia\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 9,\n",
      "      \"country\": \"Tunisia\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 10,\n",
      "      \"country\": \"Algeria\"\n",
      "    }\n",
      "  ],\n",
      "  \"disclaimer\": \"Cost of living can vary significantly within a country depending on the city and lifestyle. These rankings are based on general estimates and may not reflect individual experiences. Data is based on Numbeo's Cost of Living Index (as of October 26, 2023) and is subject to change.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "output(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8c4c4cb-746d-45bf-94f3-52a8261518d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"top_10_countries_highest_cost_of_living\": [\n",
      "    {\n",
      "      \"rank\": 1,\n",
      "      \"country\": \"Bermuda\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 2,\n",
      "      \"country\": \"Switzerland\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 3,\n",
      "      \"country\": \"Cayman Islands\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 4,\n",
      "      \"country\": \"Bahamas\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 5,\n",
      "      \"country\": \"Singapore\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 6,\n",
      "      \"country\": \"Iceland\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 7,\n",
      "      \"country\": \"Jersey\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 8,\n",
      "      \"country\": \"Luxembourg\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 9,\n",
      "      \"country\": \"Israel\"\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 10,\n",
      "      \"country\": \"Hong Kong\"\n",
      "    }\n",
      "  ],\n",
      "  \"disclaimer\": \"Cost of living rankings can vary depending on the source and methodology used. This list is based on commonly cited indices and reports, but may not be definitive.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages = chat_template.format_messages(n=\"10\", degree=\"highest\")\n",
    "output(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407cc93-e61e-4f88-a46b-de01fb3ef556",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46268e08-3f06-4f91-996d-ebd8c43f2d7f",
   "metadata": {},
   "source": [
    "## Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ef921663-0d2f-4eaa-a9e0-98dbf94d3cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What concept do you want to learn about?:  Petrophysical Analysis\n",
      "In how many paragraphs do you want this?  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "As an experienced geophysicist, I see Petrophysical Analysis as the critical bridge between our subsurface measurements and the actual reservoir properties. At its core, it's the quantitative study of rock and fluid properties within a reservoir, primarily using well log data, core samples, and sometimes seismic attributes. Its main objective is to determine how much fluid a rock can hold (porosity), how easily fluids can flow through it (permeability), and what types and proportions of fluids (oil, gas, water) are present. This analysis provides the fundamental understanding of the reservoir's storage and flow capacity.\n",
      "\n",
      "The process typically involves integrating and interpreting various data sources. Well logs, which are continuous measurements of physical properties down a borehole (like resistivity, density, neutron, and sonic), are the primary input. These logs are calibrated and validated using direct measurements from core samples, which provide ground truth for porosity, permeability, and fluid saturations. Petrophysicists then apply sophisticated models and algorithms to correct for environmental effects, identify lithology, quantify shale volume, calculate effective porosity, and determine water and hydrocarbon saturations, often accounting for complex rock-fluid interactions.\n",
      "\n",
      "Ultimately, petrophysical analysis is indispensable for robust reservoir characterization and modeling. The derived properties are crucial for accurately estimating hydrocarbon volumes (reserves), designing optimal well completion and production strategies, and making informed decisions about field development and economic viability. It allows us to understand reservoir heterogeneity, predict fluid movement, and assess the overall potential of a discovery. Without sound petrophysical analysis, our ability to effectively evaluate and manage subsurface resources would be severely limited, making it a cornerstone of modern exploration and production.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "You are an experienced Geophysicist,\n",
    "Explain the following concept: {concept} in only {nos} paragraphs, but in clear terms\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "\n",
    "chain = prompt_template | chat_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "concept = input(\"What concept do you want to learn about?: \")\n",
    "paragraphs = input(\"In how many paragraphs do you want this? \")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "output = chain.invoke({\"concept\": concept, \"nos\": paragraphs})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85013cb1-8391-421f-bb4e-e970fcc8d8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000021990A9CBE0>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d818efe5-3953-402f-8de5-115f103af2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which country?:  Brazil and Canada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "*   **Brazil:** Brasília\n",
      "*   **Canada:** Ottawa\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "What is the captal of {country}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "country = input(\"Which country?: \")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "output = chain.invoke(country)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e6178-987a-4545-859d-6aeb6b06a869",
   "metadata": {},
   "source": [
    "## Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ac24b31-06ed-4727-bd35-9e100a509ac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableSequence' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m chain2 \u001b[38;5;241m=\u001b[39m prompt2 \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# compose\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m seq_chain \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleSequentialChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mchain1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m result \u001b[38;5;241m=\u001b[39m seq_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseismic waves\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai-engine\\lib\\site-packages\\langchain_core\\load\\serializable.py:115\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai-engine\\lib\\site-packages\\langchain\\chains\\base.py:247\u001b[0m, in \u001b[0;36mChain.raise_callback_manager_deprecation\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;129m@model_validator\u001b[39m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_callback_manager_deprecation\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise deprecation warning if callback_manager is used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    250\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both callback_manager and callbacks. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback_manager is deprecated, callbacks is the preferred \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter to pass in.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai-engine\\lib\\site-packages\\pydantic\\main.py:991\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 991\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RunnableSequence' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.chains.sequential import SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# define first prompt chain\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=\"Explain the concept of {concept} simply.\",\n",
    ")\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "# define second prompt chain\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Now translate the above explanation into something a 5-year-old would understand: {text}\",\n",
    ")\n",
    "chain2 = prompt2 | llm | StrOutputParser()\n",
    "\n",
    "# compose\n",
    "seq_chain = SimpleSequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    verbose=True \n",
    ")\n",
    "\n",
    "\n",
    "result = seq_chain.invoke({\"concept\": \"seismic waves\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9e3c8-74dd-4510-8f45-cade86432297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-engine)",
   "language": "python",
   "name": "ai-engine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
